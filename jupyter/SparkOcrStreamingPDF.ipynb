{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of running OCR in streaming mode for process PDF's\n",
    "## Install spark-ocr python packge\n",
    "Need specify path to `spark-ocr-assembly-[version].jar` or `secret`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = \"\"\n",
    "license = \"\"\n",
    "version = secret.split(\"-\")[0]\n",
    "spark_ocr_jar_path = \"../../target/scala-2.11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if python -c 'import google.colab' &> /dev/null; then\n",
    "    echo \"Run on Google Colab!\"\n",
    "    echo \"Install Open JDK\"\n",
    "    apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "    java -version\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "  os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install from PYPI using secret\n",
    "%pip install spark-ocr==$version\\.spark24 --extra-index-url=https://pypi.johnsnowlabs.com/$secret --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or install from local path\n",
    "# %pip install ../../python/dist/spark-ocr-1.9.0.spark24.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkConf Configured, Starting to listen on port: 63045\n",
      "JAR PATH:/usr/local/lib/python3.7/site-packages/sparkmonitor/listener.jar\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://melnyks-mbp:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark OCR</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1275bb250>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sparkocr import start\n",
    "\n",
    "if license:\n",
    "    os.environ['JSL_OCR_LICENSE'] = license\n",
    "\n",
    "spark = start(secret=secret, jar_path=spark_ocr_jar_path)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from sparkocr.transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill path to folder with PDF's here\n",
    "dataset_path = \"data/pdfs/*.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read one file for infer schema\n",
    "pdfs_df = spark.read.format(\"binaryFile\").load(dataset_path).limit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define OCR pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform binary to image\n",
    "pdf_to_image = PdfToImage()\n",
    "pdf_to_image.setOutputCol(\"image\")\n",
    "\n",
    "# Run OCR for each region\n",
    "ocr = ImageToText()\n",
    "ocr.setInputCol(\"image\")\n",
    "ocr.setOutputCol(\"text\")\n",
    "ocr.setConfidenceThreshold(60)\n",
    "\n",
    "# OCR pipeline\n",
    "pipeline = PipelineModel(stages=[\n",
    "    pdf_to_image,\n",
    "    ocr\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define streaming pipeline and start it\n",
    "Note: each start erase previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of files in one microbatch\n",
    "maxFilesPerTrigger = 4 \n",
    "\n",
    "# read files as stream\n",
    "pdf_stream_df = spark.readStream \\\n",
    ".format(\"binaryFile\") \\\n",
    ".schema(pdfs_df.schema) \\\n",
    ".option(\"maxFilesPerTrigger\", maxFilesPerTrigger) \\\n",
    ".load(dataset_path)\n",
    "\n",
    "# process files using OCR pipoeline\n",
    "result = pipeline.transform(pdf_stream_df).withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "# store results to memory table\n",
    "query = result.writeStream \\\n",
    " .format('memory') \\\n",
    " .queryName('result') \\\n",
    " .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'a8ed6379-ea1f-42ce-bc7f-8a14aa143e9e',\n",
       " 'runId': '409d9513-36d3-487c-bd37-4438c0c039d7',\n",
       " 'name': 'result',\n",
       " 'timestamp': '2020-02-06T12:51:41.013Z',\n",
       " 'batchId': 1,\n",
       " 'numInputRows': 0,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'getOffset': 1, 'triggerExecution': 1},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'FileStreamSource[file:/Users/nmelnik/IdeaProjects/spark-ocr/workshop/jupyter/data/pdfs/*.pdf]',\n",
       "   'startOffset': {'logOffset': 0},\n",
       "   'endOffset': {'logOffset': 0},\n",
       "   'numInputRows': 0,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 0.0}],\n",
       " 'sink': {'description': 'MemorySink'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get progress of streamig job\n",
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to run for stop steraming job\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results from 'result' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of processed records (number of processed pages in results)\n",
    "spark.table(\"result\").count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+\n",
      "|           timestamp|pagenum|                path|                text|\n",
      "+--------------------+-------+--------------------+--------------------+\n",
      "|2020-02-06 19:51:...|      0|file:/Users/nmeln...| \n",
      "\n",
      "~ (OLD GOLD ST...|\n",
      "+--------------------+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show results\n",
    "spark.table(\"result\").select(\"timestamp\",\"pagenum\", \"path\", \"text\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run streaming job for storing results to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = result.select(\"text\").writeStream \\\n",
    " .format('text') \\\n",
    " .option(\"path\", \"results/\") \\\n",
    " .option(\"checkpointLocation\", \"checkpointDir\") \\\n",
    " .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '61a29c5f-ac1c-498a-a7c3-2744960cbe98',\n",
       " 'runId': 'c891f0d3-c34d-44ad-a608-81cc448a0613',\n",
       " 'name': None,\n",
       " 'timestamp': '2020-02-06T12:52:55.074Z',\n",
       " 'batchId': 1,\n",
       " 'numInputRows': 0,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'getOffset': 1, 'triggerExecution': 1},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'FileStreamSource[file:/Users/nmelnik/IdeaProjects/spark-ocr/workshop/jupyter/data/pdfs/*.pdf]',\n",
       "   'startOffset': {'logOffset': 0},\n",
       "   'endOffset': {'logOffset': 0},\n",
       "   'numInputRows': 0,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 0.0}],\n",
       " 'sink': {'description': 'FileSink[results/]'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get progress of streamig job\n",
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to run for stop steraming job\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read results from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|                    |\n",
      "|                    |\n",
      "|~ (OLD GOLD STRAI...|\n",
      "|                    |\n",
      "|control for Sampl...|\n",
      "|                    |\n",
      "|/. Length -------...|\n",
      "|Circumference~-~ ...|\n",
      "|Paper --------+--...|\n",
      "|Firmness == aa me...|\n",
      "|Draw -------~----...|\n",
      "|Weight on @ ae ee...|\n",
      "|Tipping Paper -- ...|\n",
      "|. : Labels ---- O...|\n",
      "|   Print---~--------|\n",
      "|C . Filter Length...|\n",
      "|  . Tear Tape-- Gold|\n",
      "|. Cartons --- OLD...|\n",
      "|>=, Requirements:...|\n",
      "|Laboratory ----- ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NBVAL_SKIP\n",
    "results = spark.read.format(\"text\").load(\"results/*.txt\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean results and checkpoint folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -r -f results\n",
    "rm -r -f checkpointDir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
